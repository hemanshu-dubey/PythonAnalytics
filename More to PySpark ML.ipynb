{"cells":[{"cell_type":"markdown","source":["<h2>Feature Extraction</h3>\n<h4>NLP related feature extractors</h4>"],"metadata":{}},{"cell_type":"code","source":["text_data = spark.createDataFrame([\n    ['''Machine learning can be applied to a wide variety \n        of data types, such as vectors, text, images, and \n        structured data. This API adopts the DataFrame from \n        Spark SQL in order to support a variety of data types.'''],\n    ['''DataFrame supports many basic and structured types; \n        see the Spark SQL datatype reference for a list of \n        supported types. In addition to the types listed in \n        the Spark SQL guide, DataFrame can use ML Vector types.'''],\n    ['''A DataFrame can be created either implicitly or \n        explicitly from a regular RDD. See the code examples \n        below and the Spark SQL programming guide for examples.'''],\n    ['''Columns in a DataFrame are named. The code examples \n        below use names such as \"text,\" \"features,\" and \"label.\"''']\n], ['input'])"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"code","source":["text_data.show()"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":["<p>Tokenize the text</p>"],"metadata":{}},{"cell_type":"code","source":["import pyspark.ml.feature as ft\n\ntokenizer = ft.RegexTokenizer(\n    inputCol='input', \n    outputCol='input_arr', \n    pattern='\\s+|[,.\\\"]')"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["tok = tokenizer \\\n    .transform(text_data) \\\n    .select('input_arr') \n  \ntok.take(1)"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":["<p>Use StopWordsRemover</p>"],"metadata":{}},{"cell_type":"code","source":["stopwords = ft.StopWordsRemover(\n    inputCol=tokenizer.getOutputCol(), \n    outputCol='input_stop')"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["stopwords.transform(tok).show()"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":["<p>Build NGram Model & Pipeline</p>"],"metadata":{}},{"cell_type":"code","source":["ngram = ft.NGram(n=2, \n    inputCol=stopwords.getOutputCol(), \n    outputCol=\"nGrams\")"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"code","source":["from pyspark.ml import Pipeline\npipeline = Pipeline(stages=[tokenizer, stopwords, ngram])"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"code","source":["display(pipeline.fit(text_data).transform(text_data).select('nGrams'))"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":["<h3>Discretize continues variables</h3>"],"metadata":{}},{"cell_type":"code","source":["import numpy as np\n\nx = np.arange(0,100)\nx = x /100.0 * np.pi * 4\ny = x * np.sin(x/ 1.764) + 20.1234"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"code","source":["import pyspark.sql.types as typ\nschema = typ.StructType([\n    typ.StructField('continuous_var', \n                    typ.DoubleType(), \n                    False\n   )\n])\n\ndata = spark.createDataFrame([[float(e), ] for e in y], schema=schema)"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"code","source":["data.show()"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"code","source":["help(typ.StructField)"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"code","source":["discretizer = ft.QuantileDiscretizer(\n    numBuckets=5, \n    inputCol='continuous_var', \n    outputCol='discretized')"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"code","source":["data_discretized = discretizer.fit(data).transform(data)"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"code","source":["display(data_discretized.groupby('discretized').mean('continuous_var').sort('discretized'))\n    "],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"markdown","source":["<h2>Classification</h2>"],"metadata":{}},{"cell_type":"code","source":["import pyspark.sql.types as typ\n\nlabels = [\n    ('INFANT_ALIVE_AT_REPORT', typ.IntegerType()),\n    ('BIRTH_PLACE', typ.StringType()),\n    ('MOTHER_AGE_YEARS', typ.IntegerType()),\n    ('FATHER_COMBINED_AGE', typ.IntegerType()),\n    ('CIG_BEFORE', typ.IntegerType()),\n    ('CIG_1_TRI', typ.IntegerType()),\n    ('CIG_2_TRI', typ.IntegerType()),\n    ('CIG_3_TRI', typ.IntegerType()),\n    ('MOTHER_HEIGHT_IN', typ.IntegerType()),\n    ('MOTHER_PRE_WEIGHT', typ.IntegerType()),\n    ('MOTHER_DELIVERY_WEIGHT', typ.IntegerType()),\n    ('MOTHER_WEIGHT_GAIN', typ.IntegerType()),\n    ('DIABETES_PRE', typ.IntegerType()),\n    ('DIABETES_GEST', typ.IntegerType()),\n    ('HYP_TENS_PRE', typ.IntegerType()),\n    ('HYP_TENS_GEST', typ.IntegerType()),\n    ('PREV_BIRTH_PRETERM', typ.IntegerType())\n]\n\n#creating list of StructField that eventually becomes schema\n\nschema = typ.StructType([ typ.StructField(e[0], e[1], False) for e in labels ])\n\nbirths = spark.read.csv('/FileStore/tables/5o9jitm81490770993355/births_transformed_csv-da688.gz', \n                        header=True, \n                        schema=schema)\n\n#/FileStore/tables/5o9jitm81490770993355/births_transformed_csv-da688.gz"],"metadata":{},"outputs":[],"execution_count":23},{"cell_type":"code","source":["import pyspark.sql.functions as func\n\nbirths = births.withColumn(\n    'INFANT_ALIVE_AT_REPORT', \n    func.col('INFANT_ALIVE_AT_REPORT').cast(typ.DoubleType())\n)\n\n"],"metadata":{},"outputs":[],"execution_count":24},{"cell_type":"code","source":["import pyspark.ml.classification as cl\n\nclassifier = cl.RandomForestClassifier(\n    numTrees=5, \n    maxDepth=5, \n    labelCol='INFANT_ALIVE_AT_REPORT')\n"],"metadata":{},"outputs":[],"execution_count":25},{"cell_type":"code","source":["births = births.withColumn('BIRTH_PLACE_INT', births['BIRTH_PLACE'].cast(typ.IntegerType()))\n  \nencoder = ft.OneHotEncoder(inputCol='BIRTH_PLACE_INT', outputCol='BIRTH_PLACE_VEC')\n  \nfeaturesCreator = ft.VectorAssembler(\n    inputCols=[\n        col[0] \n        for col \n        in labels[2:]] + \\\n    [encoder.getOutputCol()], \n    outputCol='features'\n)\n  \npipeline = Pipeline(stages=[encoder,featuresCreator,classifier])"],"metadata":{},"outputs":[],"execution_count":26},{"cell_type":"code","source":["births.show()"],"metadata":{},"outputs":[],"execution_count":27},{"cell_type":"code","source":["births_train, births_test = births \\\n    .randomSplit([0.7, 0.3], seed=666)"],"metadata":{},"outputs":[],"execution_count":28},{"cell_type":"code","source":["display(pipeline.fit(births_train).transform(births_test))\nmodel = pipeline.fit(births_train)\ntest = model.transform(births_test)"],"metadata":{},"outputs":[],"execution_count":29},{"cell_type":"code","source":["from pyspark.ml import evaluation as ev"],"metadata":{},"outputs":[],"execution_count":30},{"cell_type":"code","source":["evaluator = ev.BinaryClassificationEvaluator(\n    labelCol='INFANT_ALIVE_AT_REPORT')\nprint(evaluator.evaluate(test, \n    {evaluator.metricName: \"areaUnderROC\"}))\nprint(evaluator.evaluate(test, \n    {evaluator.metricName: \"areaUnderPR\"}))"],"metadata":{},"outputs":[],"execution_count":31},{"cell_type":"markdown","source":["<h3>Decision Tree - Just one of them</h3>"],"metadata":{}},{"cell_type":"code","source":["classifier = cl.DecisionTreeClassifier(\n    maxDepth=5, \n    labelCol='INFANT_ALIVE_AT_REPORT')\n\npipeline = Pipeline(stages=[\n        encoder,\n        featuresCreator, \n        classifier]\n)\n\nmodel = pipeline.fit(births_train)\ntest = model.transform(births_test)\n\nevaluator = ev.BinaryClassificationEvaluator(\n    labelCol='INFANT_ALIVE_AT_REPORT')\n\nprint(evaluator.evaluate(test, \n     {evaluator.metricName: \"areaUnderROC\"}))\nprint(evaluator.evaluate(test, \n     {evaluator.metricName: \"areaUnderPR\"}))"],"metadata":{},"outputs":[],"execution_count":33},{"cell_type":"markdown","source":["<h3>Clustering</h3>"],"metadata":{}},{"cell_type":"code","source":["from pyspark.ml import clustering as clus"],"metadata":{},"outputs":[],"execution_count":35},{"cell_type":"code","source":["kmeans = clus.KMeans(k = 5, \n    featuresCol='features')"],"metadata":{},"outputs":[],"execution_count":36},{"cell_type":"code","source":["pipeline = Pipeline(stages=[\n        encoder,\n        featuresCreator, \n        kmeans]\n)"],"metadata":{},"outputs":[],"execution_count":37},{"cell_type":"code","source":["model = pipeline.fit(births_train)"],"metadata":{},"outputs":[],"execution_count":38},{"cell_type":"code","source":["test = model.transform(births_test)\n\ndisplay(test)"],"metadata":{},"outputs":[],"execution_count":39},{"cell_type":"code","source":["test \\\n    .groupBy('prediction') \\\n    .agg({\n        '*': 'count', \n        'MOTHER_HEIGHT_IN': 'avg'\n    }).collect()"],"metadata":{},"outputs":[],"execution_count":40},{"cell_type":"code","source":["text_data = spark.createDataFrame([\n    ['''To make a computer do anything, you have to write a \n    computer program. To write a computer program, you have \n    to tell the computer, step by step, exactly what you want \n    it to do. The computer then \"executes\" the program, \n    following each step mechanically, to accomplish the end \n    goal. When you are telling the computer what to do, you \n    also get to choose how it's going to do it. That's where \n    computer algorithms come in. The algorithm is the basic \n    technique used to get the job done. Let's follow an \n    example to help get an understanding of the algorithm \n    concept.'''],\n    ['''Laptop computers use batteries to run while not \n    connected to mains. When we overcharge or overheat \n    lithium ion batteries, the materials inside start to \n    break down and produce bubbles of oxygen, carbon dioxide, \n    and other gases. Pressure builds up, and the hot battery \n    swells from a rectangle into a pillow shape. Sometimes \n    the phone involved will operate afterwards. Other times \n    it will die. And occasionally—kapow! To see what's \n    happening inside the battery when it swells, the CLS team \n    used an x-ray technology called computed tomography.'''],\n    ['''This technology describes a technique where touch \n    sensors can be placed around any side of a device \n    allowing for new input sources. The patent also notes \n    that physical buttons (such as the volume controls) could \n    be replaced by these embedded touch sensors. In essence \n    Apple could drop the current buttons and move towards \n    touch-enabled areas on the device for the existing UI. It \n    could also open up areas for new UI paradigms, such as \n    using the back of the smartphone for quick scrolling or \n    page turning.'''],\n    ['''The National Park Service is a proud protector of \n    America’s lands. Preserving our land not only safeguards \n    the natural environment, but it also protects the \n    stories, cultures, and histories of our ancestors. As we \n    face the increasingly dire consequences of climate \n    change, it is imperative that we continue to expand \n    America’s protected lands under the oversight of the \n    National Park Service. Doing so combats climate change \n    and allows all American’s to visit, explore, and learn \n    from these treasured places for generations to come. It \n    is critical that President Obama acts swiftly to preserve \n    land that is at risk of external threats before the end \n    of his term as it has become blatantly clear that the \n    next administration will not hold the same value for our \n    environment over the next four years.'''],\n    ['''The National Park Foundation, the official charitable \n    partner of the National Park Service, enriches America’s \n    national parks and programs through the support of \n    private citizens, park lovers, stewards of nature, \n    history enthusiasts, and wilderness adventurers. \n    Chartered by Congress in 1967, the Foundation grew out of \n    a legacy of park protection that began over a century \n    ago, when ordinary citizens took action to establish and \n    protect our national parks. Today, the National Park \n    Foundation carries on the tradition of early park \n    advocates, big thinkers, doers and dreamers—from John \n    Muir and Ansel Adams to President Theodore Roosevelt.'''],\n    ['''Australia has over 500 national parks. Over 28 \n    million hectares of land is designated as national \n    parkland, accounting for almost four per cent of \n    Australia's land areas. In addition, a further six per \n    cent of Australia is protected and includes state \n    forests, nature parks and conservation reserves.National \n    parks are usually large areas of land that are protected \n    because they have unspoilt landscapes and a diverse \n    number of native plants and animals. This means that \n    commercial activities such as farming are prohibited and \n    human activity is strictly monitored.''']\n], ['documents'])"],"metadata":{},"outputs":[],"execution_count":41},{"cell_type":"code","source":["tokenizer = ft.RegexTokenizer(\n    inputCol='documents', \n    outputCol='input_arr', \n    pattern='\\s+|[,.\\\"]')\n\nstopwords = ft.StopWordsRemover(\n    inputCol=tokenizer.getOutputCol(), \n    outputCol='input_stop')"],"metadata":{},"outputs":[],"execution_count":42},{"cell_type":"code","source":["stringIndexer = ft.CountVectorizer(\n    inputCol=stopwords.getOutputCol(), \n    outputCol=\"input_indexed\")\n\ntokenized = stopwords \\\n    .transform(\n        tokenizer\\\n            .transform(text_data)\n    )\n    \nstringIndexer \\\n    .fit(tokenized)\\\n    .transform(tokenized)\\\n    .select('input_indexed').take(2)"],"metadata":{},"outputs":[],"execution_count":43},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":44}],"metadata":{"name":"More to PySpark ML","notebookId":2412558471830213},"nbformat":4,"nbformat_minor":0}
