{"cells":[{"cell_type":"code","source":["%fs ls databricks-datasets/adult/adult.data"],"metadata":{},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":["<h4>Understand Data</h4>\n* age: continuous\n* workclass: Private,Self-emp-not-inc, Self-emp-inc, Federal-gov, Local-gov, State-gov, Without-pay, Never-worked\n* fnlwgt: continuous\n* education: Bachelors, Some-college, 11th, HS-grad, Prof-school, Assoc-acdm, Assoc-voc...\n* education-num: continuous\n* marital-status: Married-civ-spouse, Divorced, Never-married, Separated, Widowed, Married-spouse-absent...\n* occupation: Tech-support, Craft-repair, Other-service, Sales, Exec-managerial, Prof-specialty, Handlers-cleaners...\n* relationship: Wife, Own-child, Husband, Not-in-family, Other-relative, Unmarried\n* race: White, Asian-Pac-Islander, Amer-Indian-Eskimo, Other, Black\n* sex: Female, Male\n* capital-gain: continuous\n* capital-loss: continuous\n* hours-per-week: continuous\n* native-country: United-States, Cambodia, England, Puerto-Rico, Canada, Germany...\n* Target/Label: - <=50K, >50K"],"metadata":{}},{"cell_type":"markdown","source":["<h3>Load data</h3>"],"metadata":{}},{"cell_type":"code","source":["import pyspark.sql.types as typ\nlabels = [\n    ('age', typ.DoubleType()),\n    ('workclass', typ.StringType()),\n    ('fnlwgt', typ.DoubleType()),\n    ('education', typ.StringType()),\n    ('education-num', typ.DoubleType()),\n    ('marital-status', typ.StringType()),\n    ('occupation', typ.StringType()),\n    ('relationship', typ.StringType()),\n    ('race', typ.StringType()),\n    ('sex', typ.StringType()),\n    ('capital-gain', typ.DoubleType()),\n    ('capital-loss', typ.DoubleType()),\n    ('hours-per-week', typ.DoubleType()),\n    ('native-country', typ.StringType()),\n    ('income',typ.StringType())\n]\n\nschema = typ.StructType([ typ.StructField(e[0], e[1], False) for e in labels ])\n"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["data = spark.read.csv('databricks-datasets/adult/adult.data',header=False,schema=schema)"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["display(data)"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":["<h3>Preprocess Data</h3>\n* Converting categorical data into numerical values using StringIndex & one-hot-encoder.\n* Simplification step - we will have more than 1 stages of feature transformations, we use a Pipeline to tie the stages together."],"metadata":{}},{"cell_type":"code","source":["###One-Hot Encoding\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler\n\ncategoricalColumns = [\"workclass\", \"education\", \"marital-status\", \"occupation\", \"relationship\", \"race\", \"sex\", \"native-country\"]\nstages = [] # stages in our Pipeline\nfor categoricalCol in categoricalColumns:\n  # Category Indexing with StringIndexer\n  stringIndexer = StringIndexer(inputCol=categoricalCol, outputCol=categoricalCol+\"Index\")\n  # Use OneHotEncoder to convert categorical variables into binary SparseVectors\n  encoder = OneHotEncoder(inputCol=categoricalCol+\"Index\", outputCol=categoricalCol+\"classVec\")\n  # Add stages.  These are not run here, but will run all at once later on.\n  stages += [stringIndexer, encoder]"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["# Convert label into label indices using the StringIndexer\nlabel_stringIdx = StringIndexer(inputCol = \"income\", outputCol = \"label\")\nstages += [label_stringIdx]"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":["# Transform all features into a vector using VectorAssembler\nnumericCols = [\"age\", \"fnlwgt\", \"education-num\", \"capital-gain\", \"capital-loss\", \"hours-per-week\"]\nassemblerInputs = map(lambda c: c + \"classVec\", categoricalColumns) + numericCols\nassembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\")\nstages += [assembler]"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"code","source":["# Create a Pipeline.\npipeline = Pipeline(stages=stages)\n# Run the feature transformations.\n#  - fit() computes feature statistics as needed.\n#  - transform() actually transforms the features.\npipelineModel = pipeline.fit(data)\ndataset = pipelineModel.transform(data)\n\n# Keep relevant columns\nselectedcols = [\"label\", \"features\"] + data.columns\ndataset = dataset.select(selectedcols)\ndisplay(dataset)"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"code","source":["### Randomly split data into training and test sets. set seed for reproducibility\n(trainingData, testData) = dataset.randomSplit([0.7, 0.3], seed = 100)\nprint trainingData.count()\nprint testData.count()"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":["<h3>Fit and Evaluate Models</h3>\n* Logistic Regression"],"metadata":{}},{"cell_type":"code","source":["from pyspark.ml.classification import LogisticRegression\n\n# Create initial LogisticRegression model\nlr = LogisticRegression(labelCol=\"label\", featuresCol=\"features\", maxIter=100)\n\n# Train model with Training Data\nlrModel = lr.fit(trainingData)"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"code","source":["# Make predictions on test data using the transform() method.\n# LogisticRegression.transform() will only use the 'features' column.\npredictions = lrModel.transform(testData)"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"code","source":["# View model's predictions and probabilities of each prediction class\n# You can select any columns in the above schema to view as well. For example's sake we will choose age & occupation\nselected = predictions.select(\"label\", \"prediction\", \"probability\", \"age\", \"occupation\")\ndisplay(selected)"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"code","source":["from pyspark.ml.evaluation import BinaryClassificationEvaluator\n\n# Evaluate model\nevaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\")\nevaluator.evaluate(predictions)"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":18}],"metadata":{"name":"Kabbage Project","notebookId":1140933247921808},"nbformat":4,"nbformat_minor":0}
