{"cells":[{"cell_type":"code","source":["from pyspark.sql import *"],"metadata":{},"outputs":[],"execution_count":1},{"cell_type":"code","source":["# Create the Departments\ndepartment1 = Row(id='123456', name='Computer Science')\ndepartment2 = Row(id='789012', name='Mechanical Engineering')\ndepartment3 = Row(id='345678', name='Theater and Drama')\ndepartment4 = Row(id='901234', name='Indoor Recreation')"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"code","source":["Employee = Row(\"firstName\", \"lastName\", \"email\", \"salary\")"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["employee1 = Employee('michael', 'armbrust', 'no-reply@berkeley.edu', 100000)\nemployee2 = Employee('xiangrui', 'meng', 'no-reply@stanford.edu', 120000)\nemployee3 = Employee('matei', None, 'no-reply@waterloo.edu', 140000)\nemployee4 = Employee(None, 'wendell', 'no-reply@berkeley.edu', 160000)"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["#Connect Department with Employee\ndepartmentWithEmployees1 = Row(department=department1, employees=[employee1, employee2])\ndepartmentWithEmployees2 = Row(department=department2, employees=[employee3, employee4])\ndepartmentWithEmployees3 = Row(department=department3, employees=[employee1, employee4])\ndepartmentWithEmployees4 = Row(department=department4, employees=[employee2, employee3])"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["df1 = spark.createDataFrame([departmentWithEmployees1,departmentWithEmployees2])"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["display(df1)"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["df2 = spark.createDataFrame([departmentWithEmployees3,departmentWithEmployees4])"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["unionDF = df1.union(df2)"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":["display(unionDF)"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"code","source":["dbutils.fs.rm('/tmp/data/df.parquet',True)"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"code","source":["unionDF.write.parquet('/tmp/data/df.parquet')\n"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"code","source":["parquetDF = spark.read.parquet('/tmp/data/df.parquet')"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"code","source":["display(parquetDF)"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"code","source":["eDF = spark.createDataFrame([Row(a=1, intlist=[1,2,3], mapfield={\"a\":\"b\"})])\neDFx = spark.createDataFrame([Row(a=1,  mapfield={\"a\":\"b\"}, intlist=[1,2,3])])"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"code","source":["display(eDFx)"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"code","source":["from pyspark.sql.functions import explode\n\ndisplay(eDF.select('a',explode(eDF.intlist).alias(\"anInt\")))"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"code","source":["eDF.select(['intlist']).collect()"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"code","source":["#eDF.createGlobalTempView(\"xyz\")\ndisplay(spark.sql(\"select * from xyz\"))"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"code","source":["display(eDF.select(explode(eDF.mapfield).alias(\"k\",\"v\")))"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"code","source":["display(parquetDF)"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"code","source":["df = parquetDF.select(\"department\",explode(\"employees\").alias(\"e\"))"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"code","source":["display(df)"],"metadata":{},"outputs":[],"execution_count":23},{"cell_type":"code","source":["df = spark.createDataFrame([('abcd','123')], ['s', 'd'])\ndisplay(df)"],"metadata":{},"outputs":[],"execution_count":24},{"cell_type":"code","source":["from pyspark.sql.functions import concat\nd = df.select(concat(df.s, df.d).alias('s'))\n"],"metadata":{},"outputs":[],"execution_count":25},{"cell_type":"code","source":["from pyspark.sql.functions import abs"],"metadata":{},"outputs":[],"execution_count":26},{"cell_type":"code","source":["df = spark.createDataFrame([('abcd',-123)], ['s', 'd'])\nd = df.select(concat(df.s, abs(df.d)).alias('s'))\ndisplay(d)"],"metadata":{},"outputs":[],"execution_count":27},{"cell_type":"code","source":["explodeDF = df.selectExpr(\"e.firstName\",\"e.lastName\",\"e.email\",\"e.salary\")"],"metadata":{},"outputs":[],"execution_count":28},{"cell_type":"code","source":["display(explodeDF)"],"metadata":{},"outputs":[],"execution_count":29},{"cell_type":"code","source":["filterDF = explodeDF.filter(explodeDF.firstName == \"michael\")"],"metadata":{},"outputs":[],"execution_count":30},{"cell_type":"code","source":["display(filterDF)"],"metadata":{},"outputs":[],"execution_count":31},{"cell_type":"code","source":["display(explodeDF.sort(explodeDF.salary))"],"metadata":{},"outputs":[],"execution_count":32},{"cell_type":"code","source":["display(explodeDF)"],"metadata":{},"outputs":[],"execution_count":33},{"cell_type":"code","source":["filterDF = explodeDF.sort(explodeDF.salary)"],"metadata":{},"outputs":[],"execution_count":34},{"cell_type":"code","source":["from pyspark.sql.functions import col,asc"],"metadata":{},"outputs":[],"execution_count":35},{"cell_type":"code","source":["explodeDF.filter((col(\"firstName\") == \"xiangrui\") | (col(\"firstName\") == \"michael\" )).collect()"],"metadata":{},"outputs":[],"execution_count":36},{"cell_type":"code","source":["explodeDF.fillna(\"--\").collect()"],"metadata":{},"outputs":[],"execution_count":37},{"cell_type":"code","source":["explodeDF.filter(col(\"firstName\").isNull()).collect()"],"metadata":{},"outputs":[],"execution_count":38},{"cell_type":"code","source":["from pyspark.sql.functions import countDistinct,count"],"metadata":{},"outputs":[],"execution_count":39},{"cell_type":"code","source":["countDistinctDF = explodeDF.select(\"firstName\",\"lastName\").groupby(\"firstName\").agg(count(\"firstName\"))"],"metadata":{},"outputs":[],"execution_count":40},{"cell_type":"code","source":["display(countDistinctDF)"],"metadata":{},"outputs":[],"execution_count":41},{"cell_type":"code","source":["from pyspark.sql.window import Window"],"metadata":{},"outputs":[],"execution_count":42},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":43}],"metadata":{"name":"Understanding DataFrame","notebookId":4473555564758436},"nbformat":4,"nbformat_minor":0}
